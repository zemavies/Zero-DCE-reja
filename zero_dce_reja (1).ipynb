{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GUZHeroZHBI"
      },
      "source": [
        "# Zero-DCE for low-light image enhancement\n",
        "\n",
        "**Author:** [Soumik Rakshit](http://github.com/soumik12345)<br>\n",
        "**Date created:** 2021/09/18<br>\n",
        "**Last modified:** 2021/09/19<br>\n",
        "**Description:** Implementing Zero-Reference Deep Curve Estimation for low-light image enhancement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg4zTRGlZHBv"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "**Zero-Reference Deep Curve Estimation** or **Zero-DCE** formulates low-light image\n",
        "enhancement as the task of estimating an image-specific\n",
        "[*tonal curve*](https://en.wikipedia.org/wiki/Curve_(tonality)) with a deep neural network.\n",
        "In this example, we train a lightweight deep network, **DCE-Net**, to estimate\n",
        "pixel-wise and high-order tonal curves for dynamic range adjustment of a given image.\n",
        "\n",
        "Zero-DCE takes a low-light image as input and produces high-order tonal curves as its output.\n",
        "These curves are then used for pixel-wise adjustment on the dynamic range of the input to\n",
        "obtain an enhanced image. The curve estimation process is done in such a way that it maintains\n",
        "the range of the enhanced image and preserves the contrast of neighboring pixels. This\n",
        "curve estimation is inspired by curves adjustment used in photo editing software such as\n",
        "Adobe Photoshop where users can adjust points throughout an image’s tonal range.\n",
        "\n",
        "Zero-DCE is appealing because of its relaxed assumptions with regard to reference images:\n",
        "it does not require any input/output image pairs during training.\n",
        "This is achieved through a set of carefully formulated non-reference loss functions,\n",
        "which implicitly measure the enhancement quality and guide the training of the network.\n",
        "\n",
        "### References\n",
        "\n",
        "- [Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement](https://arxiv.org/pdf/2001.06826.pdf)\n",
        "- [Curves adjustment in Adobe Photoshop](https://helpx.adobe.com/photoshop/using/curves-adjustment.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8hqNFlOZHBz"
      },
      "source": [
        "## Downloading LOLDataset\n",
        "\n",
        "The **LoL Dataset** has been created for low-light image enhancement. It provides 485\n",
        "images for training and 15 for testing. Each image pair in the dataset consists of a\n",
        "low-light input image and its corresponding well-exposed reference image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8n31KiwZHB3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14dPS4Vb0Dcc",
        "outputId": "e6ab92bb-da8c-4b14-b318-5a3cf4ddca70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "folder /content/drive/MyDrive/Skripsi/lol_dataset/our485/low memiliki banyak files sebanyak: 2180\n",
            "folder /content/drive/MyDrive/Skripsi/lol_dataset/eval15/low memiliki banyak files sebanyak: 31\n"
          ]
        }
      ],
      "source": [
        "folder_pth = '/content/drive/MyDrive/Skripsi/lol_dataset/our485/low'\n",
        "folder_pth2 = '/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low'\n",
        "\n",
        "\n",
        "file_list = os.listdir(folder_pth)\n",
        "num_files = len(file_list)\n",
        "print(f\"folder {folder_pth} memiliki banyak files sebanyak: {num_files}\")\n",
        "\n",
        "file_list2 = os.listdir(folder_pth2)\n",
        "num_files2 = len(file_list2)\n",
        "print(f\"folder {folder_pth2} memiliki banyak files sebanyak: {num_files2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuWvhINRZHB-"
      },
      "source": [
        "## Creating a TensorFlow Dataset\n",
        "\n",
        "We use 300 low-light images from the LoL Dataset training set for training, and we use\n",
        "the remaining 185 low-light images for validation. We resize the images to size `256 x\n",
        "256` to be used for both training and validation. Note that in order to train the DCE-Net,\n",
        "we will not require the corresponding enhanced images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "curKf11_ZHCA",
        "outputId": "8b97b7b9-c2e8-421b-8715-4e42e2e0a3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Dataset berhasil dibuat\n",
            "Train Dataset: <_BatchDataset element_spec=TensorSpec(shape=(16, 256, 256, 3), dtype=tf.float32, name=None)>\n",
            "Validation Dataset: <_BatchDataset element_spec=TensorSpec(shape=(16, 256, 256, 3), dtype=tf.float32, name=None)>\n",
            "\n",
            "Jumlah Train Dataset: 125\n",
            "Jumlah Validation Dataset: 11\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "MAX_TRAIN_IMAGES = 2000\n",
        "\n",
        "\n",
        "def load_data(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def data_generator(low_light_images):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_low_light_images = sorted(glob(\"/content/drive/MyDrive/Skripsi/lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
        "val_low_light_images = sorted(glob(\"/content/drive/MyDrive/Skripsi/lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
        "test_low_light_images = sorted(glob(\"/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/*\"))\n",
        "\n",
        "\n",
        "train_dataset = data_generator(train_low_light_images)\n",
        "val_dataset = data_generator(val_low_light_images)\n",
        "\n",
        "print(\"Tensorflow Dataset berhasil dibuat\")\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Validation Dataset:\", val_dataset)\n",
        "\n",
        "print()\n",
        "print(\"Jumlah Train Dataset:\", len(list(train_dataset)))\n",
        "print(\"Jumlah Validation Dataset:\", len(list(val_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzgd0g4xZHCF"
      },
      "source": [
        "## The Zero-DCE Framework\n",
        "\n",
        "The goal of DCE-Net is to estimate a set of best-fitting light-enhancement curves\n",
        "(LE-curves) given an input image. The framework then maps all pixels of the input’s RGB\n",
        "channels by applying the curves iteratively to obtain the final enhanced image.\n",
        "\n",
        "### Understanding light-enhancement curves\n",
        "\n",
        "A ligh-enhancement curve is a kind of curve that can map a low-light image\n",
        "to its enhanced version automatically,\n",
        "where the self-adaptive curve parameters are solely dependent on the input image.\n",
        "When designing such a curve, three objectives should be taken into account:\n",
        "\n",
        "- Each pixel value of the enhanced image should be in the normalized range `[0,1]`, in order to\n",
        "avoid information loss induced by overflow truncation.\n",
        "- It should be monotonous, to preserve the contrast between neighboring pixels.\n",
        "- The shape of this curve should be as simple as possible,\n",
        "and the curve should be differentiable to allow backpropagation.\n",
        "\n",
        "The light-enhancement curve is separately applied to three RGB channels instead of solely on the\n",
        "illumination channel. The three-channel adjustment can better preserve the inherent color and reduce\n",
        "the risk of over-saturation.\n",
        "\n",
        "![](https://li-chongyi.github.io/Zero-DCE_files/framework.png)\n",
        "\n",
        "### DCE-Net\n",
        "\n",
        "The DCE-Net is a lightweight deep neural network that learns the mapping between an input\n",
        "image and its best-fitting curve parameter maps. The input to the DCE-Net is a low-light\n",
        "image while the outputs are a set of pixel-wise curve parameter maps for corresponding\n",
        "higher-order curves. It is a plain CNN of seven convolutional layers with symmetrical\n",
        "concatenation. Each layer consists of 32 convolutional kernels of size 3×3 and stride 1\n",
        "followed by the ReLU activation function. The last convolutional layer is followed by the\n",
        "Tanh activation function, which produces 24 parameter maps for 8 iterations, where each\n",
        "iteration requires three curve parameter maps for the three channels.\n",
        "\n",
        "![](https://i.imgur.com/HtIg34W.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wJ47PaVUZHCH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_dce_net():\n",
        "    input_img = keras.Input(shape=[None, None, 3])\n",
        "    conv1 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(input_img)\n",
        "    conv2 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(conv1)\n",
        "    conv3 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(conv2)\n",
        "    conv4 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(conv3)\n",
        "    int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])\n",
        "    conv5 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(int_con1)\n",
        "    int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])\n",
        "    conv6 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(int_con2)\n",
        "    int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n",
        "    x_r = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(\n",
        "        int_con3\n",
        "    )\n",
        "    return keras.Model(inputs=input_img, outputs=x_r)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sq-Z6U8ZHCI"
      },
      "source": [
        "## Loss functions\n",
        "\n",
        "To enable zero-reference learning in DCE-Net, we use a set of differentiable\n",
        "zero-reference losses that allow us to evaluate the quality of enhanced images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQylXOSqZHCM"
      },
      "source": [
        "### Color constancy loss\n",
        "\n",
        "The *color constancy loss* is used to correct the potential color deviations in the\n",
        "enhanced image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_-q8A_DMZHCM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def color_constancy_loss(x):\n",
        "    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n",
        "    mr, mg, mb = mean_rgb[:, :, :, 0], mean_rgb[:, :, :, 1], mean_rgb[:, :, :, 2]\n",
        "    d_rg = tf.square(mr - mg)\n",
        "    d_rb = tf.square(mr - mb)\n",
        "    d_gb = tf.square(mb - mg)\n",
        "    return tf.sqrt(tf.square(d_rg) + tf.square(d_rb) + tf.square(d_gb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNq71FewZHCN"
      },
      "source": [
        "### Exposure loss\n",
        "\n",
        "To restrain under-/over-exposed regions, we use the *exposure control loss*.\n",
        "It measures the distance between the average intensity value of a local region\n",
        "and a preset well-exposedness level (set to `0.6`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1VpP1RTyZHCN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def exposure_loss(x, mean_val=0.6):\n",
        "    x = tf.reduce_mean(x, axis=3, keepdims=True)\n",
        "    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n",
        "    return tf.reduce_mean(tf.square(mean - mean_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLityoLkZHCN"
      },
      "source": [
        "### Illumination smoothness loss\n",
        "\n",
        "To preserve the monotonicity relations between neighboring pixels, the\n",
        "*illumination smoothness loss* is added to each curve parameter map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PpoDQSNDZHCO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def illumination_smoothness_loss(x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    h_x = tf.shape(x)[1]\n",
        "    w_x = tf.shape(x)[2]\n",
        "    count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n",
        "    count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n",
        "    h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\n",
        "    w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\n",
        "    batch_size = tf.cast(batch_size, dtype=tf.float32)\n",
        "    count_h = tf.cast(count_h, dtype=tf.float32)\n",
        "    count_w = tf.cast(count_w, dtype=tf.float32)\n",
        "    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUeFTVdZZHCO"
      },
      "source": [
        "### Spatial consistency loss\n",
        "\n",
        "The *spatial consistency loss* encourages spatial coherence of the enhanced image by\n",
        "preserving the contrast between neighboring regions across the input image and its enhanced version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w99Bgis6ZHCO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SpatialConsistencyLoss(keras.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(reduction=\"none\")\n",
        "\n",
        "        self.left_kernel = tf.constant(\n",
        "            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
        "        )\n",
        "        self.right_kernel = tf.constant(\n",
        "            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n",
        "        )\n",
        "        self.up_kernel = tf.constant(\n",
        "            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
        "        )\n",
        "        self.down_kernel = tf.constant(\n",
        "            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "\n",
        "        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n",
        "        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n",
        "        original_pool = tf.nn.avg_pool2d(\n",
        "            original_mean, ksize=4, strides=4, padding=\"VALID\"\n",
        "        )\n",
        "        enhanced_pool = tf.nn.avg_pool2d(\n",
        "            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n",
        "        )\n",
        "\n",
        "        d_original_left = tf.nn.conv2d(\n",
        "            original_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "        d_original_right = tf.nn.conv2d(\n",
        "            original_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "        d_original_up = tf.nn.conv2d(\n",
        "            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "        d_original_down = tf.nn.conv2d(\n",
        "            original_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        d_enhanced_left = tf.nn.conv2d(\n",
        "            enhanced_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "        d_enhanced_right = tf.nn.conv2d(\n",
        "            enhanced_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "        d_enhanced_up = tf.nn.conv2d(\n",
        "            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "        d_enhanced_down = tf.nn.conv2d(\n",
        "            enhanced_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        d_left = tf.square(d_original_left - d_enhanced_left)\n",
        "        d_right = tf.square(d_original_right - d_enhanced_right)\n",
        "        d_up = tf.square(d_original_up - d_enhanced_up)\n",
        "        d_down = tf.square(d_original_down - d_enhanced_down)\n",
        "        return d_left + d_right + d_up + d_down\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM8raS59ZHCO"
      },
      "source": [
        "### Deep curve estimation model\n",
        "\n",
        "We implement the Zero-DCE framework as a Keras subclassed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gE9itvCIZHCP"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ZeroDCE(keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dce_model = build_dce_net()\n",
        "\n",
        "    def compile(self, learning_rate, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n",
        "\n",
        "    def get_enhanced_image(self, data, output):\n",
        "        r1 = output[:, :, :, :3]\n",
        "        r2 = output[:, :, :, 3:6]\n",
        "        r3 = output[:, :, :, 6:9]\n",
        "        r4 = output[:, :, :, 9:12]\n",
        "        r5 = output[:, :, :, 12:15]\n",
        "        r6 = output[:, :, :, 15:18]\n",
        "        r7 = output[:, :, :, 18:21]\n",
        "        r8 = output[:, :, :, 21:24]\n",
        "        x = data + r1 * (tf.square(data) - data)\n",
        "        x = x + r2 * (tf.square(x) - x)\n",
        "        x = x + r3 * (tf.square(x) - x)\n",
        "        enhanced_image = x + r4 * (tf.square(x) - x)\n",
        "        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n",
        "        x = x + r6 * (tf.square(x) - x)\n",
        "        x = x + r7 * (tf.square(x) - x)\n",
        "        enhanced_image = x + r8 * (tf.square(x) - x)\n",
        "        return enhanced_image\n",
        "\n",
        "    def call(self, data):\n",
        "        dce_net_output = self.dce_model(data)\n",
        "        return self.get_enhanced_image(data, dce_net_output)\n",
        "\n",
        "    def compute_losses(self, data, output):\n",
        "        enhanced_image = self.get_enhanced_image(data, output)\n",
        "        loss_illumination = 200 * illumination_smoothness_loss(output)\n",
        "        loss_spatial_constancy = tf.reduce_mean(\n",
        "            self.spatial_constancy_loss(enhanced_image, data)\n",
        "        )\n",
        "        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n",
        "        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n",
        "        total_loss = (\n",
        "            loss_illumination\n",
        "            + loss_spatial_constancy\n",
        "            + loss_color_constancy\n",
        "            + loss_exposure\n",
        "        )\n",
        "        return {\n",
        "            \"total_loss\": total_loss,\n",
        "            \"illumination_smoothness_loss\": loss_illumination,\n",
        "            \"spatial_constancy_loss\": loss_spatial_constancy,\n",
        "            \"color_constancy_loss\": loss_color_constancy,\n",
        "            \"exposure_loss\": loss_exposure,\n",
        "        }\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self.dce_model(data)\n",
        "            losses = self.compute_losses(data, output)\n",
        "        gradients = tape.gradient(\n",
        "            losses[\"total_loss\"], self.dce_model.trainable_weights\n",
        "        )\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n",
        "        return losses\n",
        "\n",
        "    def test_step(self, data):\n",
        "        output = self.dce_model(data)\n",
        "        return self.compute_losses(data, output)\n",
        "\n",
        "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
        "        \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n",
        "        self.dce_model.save_weights(\n",
        "            filepath, overwrite=overwrite, save_format=save_format, options=options\n",
        "        )\n",
        "\n",
        "    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n",
        "        \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n",
        "        self.dce_model.load_weights(\n",
        "            filepath=filepath,\n",
        "            by_name=by_name,\n",
        "            skip_mismatch=skip_mismatch,\n",
        "            options=options,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFNw7gHqZHCP"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjZQjDKUZHCQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Callback untuk menyimpan model terbaik\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model.h5',  # Menyimpan model terbaik ke file\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Callback untuk menghentikan pelatihan jika tidak ada peningkatan\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    patience=10,  # Berhenti setelah 10 epocek tanpa peningkatan\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "zero_dce_model = ZeroDCE()\n",
        "zero_dce_model.compile(learning_rate=1e-4)\n",
        "history = zero_dce_model.fit(train_dataset, validation_data=val_dataset, epochs=210)\n",
        "\n",
        "\n",
        "def plot_result(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_result(\"total_loss\")\n",
        "plot_result(\"illumination_smoothness_loss\")\n",
        "plot_result(\"spatial_constancy_loss\")\n",
        "plot_result(\"color_constancy_loss\")\n",
        "plot_result(\"exposure_loss\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_duration = end_time - start_time\n",
        "print(\"Total time: {:.2f} seconds \".format(training_duration))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feighup4ZHCQ"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8unZD_f8ZHCR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_results(images, titles, figure_size=(12, 12)):\n",
        "    fig = plt.figure(figsize=figure_size)\n",
        "    for i in range(len(images)):\n",
        "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
        "        _ = plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def infer(original_image):\n",
        "    image = keras.utils.img_to_array(original_image)\n",
        "    image = image.astype(\"float32\") / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    output_image = zero_dce_model(image)\n",
        "    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n",
        "    output_image = Image.fromarray(output_image.numpy())\n",
        "    return output_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnEawpsRp-On"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnewY6ddsaXq"
      },
      "outputs": [],
      "source": [
        "# Set up optimizer, loss function, and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile the loaded model\n",
        "zero_dce_model.compile(learning_rate=0.001, optimizer=optimizer, loss=loss_function, metrics=metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGOTTxWoqMn2"
      },
      "outputs": [],
      "source": [
        "model_name = \"model7_zerodce_210epoch.h5\"\n",
        "zero_dce_model.dce_model.save(\"/content/drive/MyDrive/Skripsi/model/{}\".format(model_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEksB3CoqUKT",
        "outputId": "9576ec72-c737-4c62-81b6-b1372a33108a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjxBHjdoZHCR"
      },
      "source": [
        "### Inference on test images\n",
        "\n",
        "We compare the test images from LOLDataset enhanced by MIRNet with images enhanced via\n",
        "the `PIL.ImageOps.autocontrast()` function.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/low-light-image-enhancement) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/low-light-image-enhancement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ire1v5ZHCR"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageOps\n",
        "for val_image_file in test_low_light_images:\n",
        "    original_image = Image.open(val_image_file)\n",
        "    enhanced_image = infer(original_image)\n",
        "    plot_results(\n",
        "        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n",
        "        [\"Original\", \"PIL Autocontrast\", \"Enhanced\"],\n",
        "        (20, 12),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OAADjqAI2e5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "#my_img = ['/content/drive/MyDrive/Skripsi/test_img/cropped_darksun_1_256x256_400.jpg','/content/drive/MyDrive/Skripsi/test_img/cropped_nami_dark_1_256x256_400.jpg']\n",
        "my_img = [ '/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/2015_03109.jpg']\n",
        "for image_path in my_img:\n",
        "    img = cv2.imread(image_path)\n",
        "    cv2_imshow(img)\n",
        "    cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "print(tf.shape(my_img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQWf9l4JStzS"
      },
      "source": [
        "## Inference Test ke Image sendiri\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga2nKiMwJ34h"
      },
      "outputs": [],
      "source": [
        "my_img =  ['/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/2015_04431.jpg','/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/2015_04434.jpg','/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/2015_06274.jpg','/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/2015_06276.jpg','/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/2015_06268.jpg','/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/2015_03110.jpg','/content/drive/MyDrive/Skripsi/lol_dataset/eval15/low/2015_03145.jpg']\n",
        "\n",
        "for val_image_file in my_img:\n",
        "    original_image = Image.open(val_image_file)\n",
        "    enhanced_image = infer(original_image)\n",
        "    plot_results(\n",
        "        [original_image, enhanced_image],\n",
        "        [\"Original\",\"Enhanced\"],\n",
        "        (20, 12),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI4rZbD-SzJB"
      },
      "source": [
        "## Inference test ke Video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tanpa BGR2RGB\n",
        "\n",
        "import cv2\n",
        "\n",
        "# Buka video sebagai input\n",
        "input_video_path = '/content/drive/MyDrive/Skripsi/test_img/low_vid.mp4'\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Menyiapkan video output\n",
        "output_video_path = '/content/drive/MyDrive/Skripsi/test_img/low_vid_test210epoch.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Anda dapat mengganti kode fourcc sesuai dengan format video yang diinginkan\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, 30, (frame_width, frame_height))  # Gantilah frame_width dan frame_height sesuai dengan resolusi video Anda\n",
        "\n",
        "# Loop melalui setiap frame dalam video\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Konversi frame menjadi format yang dapat diolah oleh model Zero-DCE\n",
        "    original_image = Image.fromarray(frame)\n",
        "    enhanced_image = infer(original_image)\n",
        "\n",
        "    # Ubah hasil enhancement kembali ke format yang dapat ditulis ke video\n",
        "    enhanced_frame = np.array(enhanced_image)\n",
        "\n",
        "    # Tulis bingkai hasil perbaikan ke video keluaran\n",
        "    out.write(enhanced_frame)\n",
        "\n",
        "# Tutup video input dan output\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"Video disimpan pada file: {output_video_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z0qTjrDs60F",
        "outputId": "b09e3d84-c710-4e25-f0a6-191e208bd334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video disimpan pada file: /content/drive/MyDrive/Skripsi/test_img/low_vid_test210epoch.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Buka video sebagai input\n",
        "input_video_path = '/content/drive/MyDrive/Skripsi/test_img/low_vid.mp4'  # Ganti dengan path video input Anda\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Menyiapkan video output\n",
        "output_video_path = '/content/drive/MyDrive/Skripsi/test_img/low_vid_210epoch.mp4'  # Ganti dengan path video output yang diinginkan\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Anda dapat mengganti kode fourcc sesuai dengan format video yang diinginkan\n",
        "frame_width = int(cap.get(3))  # Dapatkan lebar frame video asli\n",
        "frame_height = int(cap.get(4))  # Dapatkan tinggi frame video asli\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, 30, (frame_width, frame_height))\n",
        "\n",
        "# Loop melalui setiap frame dalam video\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Ubah format BGR ke RGB (model Zero-DCE mungkin memerlukan RGB)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    original_image = Image.fromarray(frame_rgb, 'RGB')\n",
        "\n",
        "    # Lakukan inferensi menggunakan model Zero-DCE\n",
        "    enhanced_image = infer(original_image)\n",
        "\n",
        "    # Ubah hasil enhancement kembali ke format yang dapat ditulis ke video\n",
        "    enhanced_frame = np.array(enhanced_image)\n",
        "\n",
        "    # Tulis frame hasil perbaikan ke video keluaran\n",
        "    out.write(enhanced_frame)\n",
        "\n",
        "# Tutup video input dan output\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"Video berhasil disimpan di: {output_video_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJtu2MG6r_FY",
        "outputId": "8025ea1d-1637-4954-8bba-dde759d5a2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video berhasil disimpan di: /content/drive/MyDrive/Skripsi/test_img/low_vid_210epoch.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1Q751rgT5YD"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('/content/drive/MyDrive/Skripsi/test_img/low_vid.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PorvSKH9XMHm"
      },
      "outputs": [],
      "source": [
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}